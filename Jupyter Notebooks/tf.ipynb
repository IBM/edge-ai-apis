{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Compression API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook showcases the API that performs Model Compression via structured channel pruning for both TensorFlow and PyTorch models. Structured pruning performs a one-shot pruning and returns the model with a user defined sparcity. Re-training will be performed by the user. For quick reference, developers can read Pruning Filters for Efficient ConvNets --> https://arxiv.org/abs/1608.08710"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a TensorFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AlexNet\n",
    "# source https://towardsdatascience.com/implementing-alexnet-cnn-architecture-using-tensorflow-2-0-and-keras-2113e090ad98\n",
    "# baseline cnn model for AlexNet\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.python.framework import type_spec as type_spec_module\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from ode.tf_pruner import TfPruner\n",
    "from ode.tf_quantizer import TfQuantizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, Train, and Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a load_dataset() method, as well as a pre-processor in prep_pixels(). \n",
    "We limit the dataset to only 1000 elements to keep training short and the following statements can be omitted\n",
    "from code below.\n",
    "\n",
    "    trainX = trainX[:1000]\n",
    "    trainY = trainY[:1000]\n",
    "    testX = testX[:1000]\n",
    "    testY = testY[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    # load dataset\n",
    "    (trainX, trainY), (testX, testY) = mnist.load_data()\n",
    "    # reshape dataset to have a single channel\n",
    "    trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
    "    testX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
    "    # one hot encode target values\n",
    "    trainY = to_categorical(trainY)\n",
    "    testY = to_categorical(testY)\n",
    "\n",
    "    trainX = trainX[:1000]\n",
    "    trainY = trainY[:1000]\n",
    "    testX = testX[:1000]\n",
    "    testY = testY[:1000]\n",
    "\n",
    "    print(f'trainX.shape: {trainX.shape}')\n",
    "    return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale pixels\n",
    "def prep_pixels(train, test):\n",
    "    # convert from integers to floats\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "    # normalize to range 0-1\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    # return normalized images\n",
    "    return train_norm, test_norm\n",
    "\n",
    "\n",
    "def compile_model(model):\n",
    "    opt = SGD(lr=0.01, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "def define_model():\n",
    "    \"\"\"Model with activation layers\"\"\"\n",
    "    model = keras.Sequential() #.to(device=device)\n",
    "    model.add(keras.layers.Conv2D(32, (3, 3), kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add(keras.layers.Conv2D(64, (3, 3), kernel_initializer='he_uniform'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.Conv2D(32, (3, 3), kernel_initializer='he_uniform'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "     # compile model\n",
    "    compile_model(model)\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to initialize the timer, dataset and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_milli_time = lambda: int(round(time.time() * 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepares cross validation\n",
    "kfold = KFold(5, shuffle=True, random_state=1)\n",
    "\n",
    "train_ds_X, train_ds_Y, test_ds_X, test_ds_Y = load_dataset()\n",
    "train_ds_X, test_ds_X = prep_pixels(train_ds_X, test_ds_X)\n",
    "\n",
    "# Define model\n",
    "model = define_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now can train the model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enumerate Splits\n",
    "for train_ix, test_ix in kfold.split(train_ds_X):\n",
    "    \n",
    "    # select rows for train and test\n",
    "    trainX, trainY, testX, testY = train_ds_X[train_ix], train_ds_Y[train_ix], test_ds_X[test_ix], test_ds_Y[test_ix]\n",
    "    # fit model\n",
    "    history = model.fit(trainX, trainY, epochs=10, batch_size=32, validation_data=(testX, testY), verbose=0)\n",
    "    # evaluate model\n",
    "\n",
    "    _, acc = model.evaluate(testX, testY, verbose=0)\n",
    "\n",
    "    print('> %.3f' % (acc * 100.0))\n",
    "\n",
    "    latest_trainX = trainX\n",
    "    latest_trainY = trainY\n",
    "    latest_testX = testX\n",
    "    latest_testY = testY\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep a subset of the dataset in latest_* for evaluation.\n",
    "\n",
    "Next, we time the prediction for comparison and print the original model stats as well as summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = 0.0\n",
    "for i in range(0, len(latest_testX)):\n",
    "    \n",
    "    img = latest_testX[i]\n",
    "    img = (np.expand_dims(img,0))\n",
    "\n",
    "    t1 = current_milli_time()\n",
    "    prediction = model.predict(img)\n",
    "    t2 += current_milli_time() - t1\n",
    "\n",
    "t2 /= float(len(latest_testX))\n",
    "\n",
    "print('> Original Model Accuracy: %.3f' % (acc * 100.0))\n",
    "print('> Original Model Inference Time: {}'.format(t2))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will save the full model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mnist_base.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now please following the next stage of tutorial as described in the learning path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0eb968810f4afd3dbf3c364fc671603f3fcedc401136b380925da47f830861db"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
